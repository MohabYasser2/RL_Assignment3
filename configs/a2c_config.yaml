# A2C Configuration for different environments
# Advantage Actor-Critic algorithm hyperparameters

CartPole-v1:
  discount_factor: 0.99
  learning_rate: 0.00025
  n_steps: 20
  entropy_coef: 0.02
  value_loss_coef: 0.5
  hidden_dim: 128
  max_grad_norm: 0.5
  gae_lambda: 1.0

  # Convergence
  convergence_threshold: 475
  convergence_window: 100
  min_episodes: 100
  max_episodes: 5000

Acrobot-v1:
  discount_factor: 0.99
  learning_rate: 0.00025
  n_steps: 20
  entropy_coef: 0.02
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  hidden_dim: 128
  gae_lambda: 1.0

  # Convergence
  convergence_threshold: -90.0
  convergence_window: 100
  min_episodes: 100
  max_episodes: 2000
  log_interval: 20

MountainCar-v0:
  discount_factor: 0.99
  learning_rate: 0.001
  n_steps: 50
  entropy_coef: 0.1
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  hidden_dim: 256
  gae_lambda: 0.99

  # Convergence
  convergence_threshold: -110.0
  convergence_window: 100
  min_episodes: 100
  max_episodes: 5000
  log_interval: 50

Pendulum-v1:
  discount_factor: 0.99
  learning_rate: 0.0003 # Base learning rate (fallback)
  actor_lr: 0.0002 # Actor learning rate for separate networks
  critic_lr: 0.0005 # Critic learning rate (higher for faster value learning)
  n_steps: 20 # Longer rollouts for better value estimates
  entropy_coef: 0.1 # Higher entropy for discrete exploration
  value_loss_coef: 0.5
  max_grad_norm: 0.5
  hidden_dim: 256 # Large network for better representation
  gae_lambda: 0.95 # Standard GAE

  # Convergence
  convergence_threshold: -250.0 # Target for discrete Pendulum
  convergence_window: 100
  min_episodes: 200
  max_episodes: 3000
  log_interval: 20

  continuous: false # Using discrete action space (wrapper applied by default)
